# Continue Configuration for Local Ollama Models
# Location: ~/.continue/config.yaml or .continue/config.yaml

models:
  # Method 1: Using Hub Model Blocks (Recommended)
  # These are pre-configured blocks from Continue Mission Control
  - uses: ollama/deepseek-coder-6.7b
  
  # Method 2: Manual Configuration for DeepSeek Coder
  - name: DeepSeek Coder 6.7b
    provider: ollama
    model: deepseek-coder:6.7b  # Must match exactly what 'ollama list' shows
    apiBase: http://localhost:11434
    contextLength: 16384
    completionOptions:
      temperature: 0.7
      top_p: 0.9
      top_k: 40
      num_predict: 2048
    roles:
      - chat
      - edit
      - apply
    capabilities:
      - tool_use  # Required for agent mode

  # Method 3: Llama 3.2 for faster tasks
  - name: Llama 3.2 Fast
    provider: ollama
    model: llama3.2:latest
    apiBase: http://localhost:11434
    contextLength: 131072  # Large context window
    completionOptions:
      temperature: 0.7
      num_predict: 2048
    roles:
      - chat
      - autocomplete
      - edit
    capabilities:
      - tool_use

  # Method 4: Autodetect (Scans for all available Ollama models)
  - name: Autodetect Ollama Models
    provider: ollama
    model: AUTODETECT
    apiBase: http://localhost:11434
    roles:
      - chat
      - edit
      - apply
      - rerank
      - autocomplete

# Optional: Configure default model
defaultModel: "DeepSeek Coder 6.7b"

# Optional: System message for all models
systemMessage: |
  You are an expert coding assistant working on the COI (Conflict of Interest) System.
  Follow project coding standards and verify all code references before using them.
